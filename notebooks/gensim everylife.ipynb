{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/topper/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datacode.retrieve_data import pull_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"LICENSE\")))\n",
    "interim_data_path = os.path.join(root_dir, \"data/interim\")\n",
    "processed_data_path = os.path.join(root_dir, \"data/processed\")\n",
    "raw_data_path = os.path.join(root_dir, \"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = load_files(os.path.join(raw_data_path, \"aclImdb/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = imdb_train.data, imdb_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_df = pd.DataFrame({'text': text_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"Zero Day leads you to think, even re-think w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Words can\\'t describe how bad this movie is....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Everyone plays their part pretty well in thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'There are a lot of highly talented filmmaker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'I\\'ve just had the evidence that confirmed m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  b\"Zero Day leads you to think, even re-think w...\n",
       "1  b'Words can\\'t describe how bad this movie is....\n",
       "2  b'Everyone plays their part pretty well in thi...\n",
       "3  b'There are a lot of highly talented filmmaker...\n",
       "4  b'I\\'ve just had the evidence that confirmed m..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLING FOR TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_df_samp = text_train_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['film','movie','picture','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Detecting phrases based on collocation counts bi_min=15, tri_min=10\n",
    "def bigrams(words, bi_min=1, tri_min=1):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \"\"\"\n",
    "    Get Bigram Model, Corpus, id2word mapping\n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. REVERT OFF SAMPLE AFTER TEST\n",
    "train_corpus4, train_id2word4, bigram_train4 = get_corpus(text_train_df_samp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the files\n",
    "corpus_path = os.path.join(interim_data_path, \"train_corpus4.pkl\")\n",
    "id2word_path = os.path.join(interim_data_path, \"train_id2word4.pkl\")\n",
    "bigram_train_path = os.path.join(interim_data_path, \"bigram_train4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the files\n",
    "with open(corpus_path, 'wb') as f:\n",
    "    pickle.dump(train_corpus4, f)\n",
    "with open(id2word_path, 'wb') as f:\n",
    "    pickle.dump(train_id2word4, f)\n",
    "with open(bigram_train_path, 'wb') as f:\n",
    "    pickle.dump(bigram_train4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in previously stored files\n",
    "with open(corpus_path, 'rb') as f:\n",
    "    train_corpus4 = pickle.load(f)\n",
    "with open(id2word_path, 'rb') as f:\n",
    "    train_id2word4 = pickle.load(f)\n",
    "with open(bigram_train_path, 'rb') as f:\n",
    "    bigram_train4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "model_log = os.path.join(processed_data_path, \"logs/lda_model.log\")\n",
    "model_data = os.path.join(processed_data_path, \"model_data/lda_train4.model\")\n",
    "logging.basicConfig(filename=model_log, format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus4,\n",
    "                           num_topics=10,\n",
    "                           id2word=train_id2word4,\n",
    "                           chunksize=100,\n",
    "                           workers=2, # Num. Processing Cores - 1\n",
    "                           passes=20,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "    lda_train4.save(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"bad\" + 0.015*\"even\" + 0.010*\"would\" + 0.010*\"story\" + 0.009*\"get\" + 0.008*\"good\" + 0.008*\"make\" + 0.008*\"scene\" + 0.008*\"seen\" + 0.007*\"lot\" + 0.007*\"scenes\" + 0.007*\"terrible\" + 0.006*\"guy\" + 0.006*\"plot\" + 0.006*\"know\"'),\n",
       " (1,\n",
       "  '0.027*\"original\" + 0.019*\"book\" + 0.013*\"plot\" + 0.013*\"bad\" + 0.011*\"cheesy\" + 0.011*\"script\" + 0.010*\"watch\" + 0.009*\"movies\" + 0.009*\"make\" + 0.008*\"novel\" + 0.008*\"sequel\" + 0.007*\"hilarious\" + 0.007*\"read\" + 0.007*\"characters\" + 0.007*\"acting\"'),\n",
       " (2,\n",
       "  '0.012*\"even\" + 0.011*\"made\" + 0.010*\"would\" + 0.010*\"really\" + 0.008*\"think\" + 0.008*\"good\" + 0.007*\"movies\" + 0.007*\"see\" + 0.007*\"also\" + 0.006*\"people\" + 0.006*\"get\" + 0.006*\"give\" + 0.006*\"seen\" + 0.006*\"way\" + 0.006*\"make\"'),\n",
       " (3,\n",
       "  '0.029*\"show\" + 0.015*\"good\" + 0.012*\"episode\" + 0.012*\"could\" + 0.010*\"funny\" + 0.008*\"series\" + 0.008*\"time\" + 0.008*\"really\" + 0.008*\"would\" + 0.007*\"love\" + 0.007*\"characters\" + 0.007*\"great\" + 0.006*\"see\" + 0.006*\"way\" + 0.006*\"many\"'),\n",
       " (4,\n",
       "  '0.016*\"good\" + 0.016*\"people\" + 0.013*\"watch\" + 0.012*\"bad\" + 0.012*\"movies\" + 0.011*\"dvd\" + 0.011*\"think\" + 0.010*\"even\" + 0.009*\"seen\" + 0.009*\"made\" + 0.009*\"characters\" + 0.009*\"say\" + 0.009*\"well\" + 0.008*\"scenes\" + 0.008*\"features\"'),\n",
       " (5,\n",
       "  '0.014*\"story\" + 0.012*\"great\" + 0.008*\"see\" + 0.008*\"new\" + 0.007*\"really\" + 0.007*\"music\" + 0.007*\"world\" + 0.006*\"even\" + 0.006*\"know\" + 0.006*\"old\" + 0.006*\"much\" + 0.006*\"lot\" + 0.006*\"filmed\" + 0.005*\"many\" + 0.005*\"feel\"'),\n",
       " (6,\n",
       "  '0.009*\"character\" + 0.009*\"story\" + 0.008*\"would\" + 0.008*\"much\" + 0.008*\"also\" + 0.008*\"well\" + 0.007*\"great\" + 0.007*\"really\" + 0.007*\"characters\" + 0.007*\"love\" + 0.007*\"time\" + 0.006*\"though\" + 0.006*\"man\" + 0.006*\"work\" + 0.006*\"seen\"'),\n",
       " (7,\n",
       "  '0.011*\"good\" + 0.009*\"really\" + 0.009*\"story\" + 0.009*\"great\" + 0.008*\"well\" + 0.008*\"also\" + 0.006*\"time\" + 0.006*\"much\" + 0.006*\"plot\" + 0.006*\"first\" + 0.006*\"make\" + 0.005*\"still\" + 0.005*\"actually\" + 0.005*\"game\" + 0.005*\"see\"'),\n",
       " (8,\n",
       "  '0.020*\"family\" + 0.012*\"see\" + 0.012*\"films\" + 0.012*\"even\" + 0.010*\"much\" + 0.010*\"story\" + 0.010*\"james\" + 0.009*\"work\" + 0.009*\"character\" + 0.008*\"wonderful\" + 0.007*\"often\" + 0.007*\"kind\" + 0.007*\"ending\" + 0.007*\"however\" + 0.007*\"take\"'),\n",
       " (9,\n",
       "  '0.014*\"little\" + 0.011*\"would\" + 0.010*\"way\" + 0.010*\"love\" + 0.010*\"woman\" + 0.009*\"time\" + 0.009*\"even\" + 0.008*\"women\" + 0.007*\"never\" + 0.007*\"still\" + 0.007*\"much\" + 0.007*\"life\" + 0.007*\"parents\" + 0.007*\"another\" + 0.006*\"something\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train4.print_topics(20,num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(text_train)):\n",
    "    top_topics = lda_train4.get_document_topics(train_corpus4[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "    topic_vec.extend([len(text_train_df.iloc[i].text)]) # length review\n",
    "    train_vecs.append(topic_vec)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)\n",
    "    \n",
    "with open('X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.795 +- 0.006\n",
      "Logisitic Regression SGD Val f1: 0.789 +- 0.007\n",
      "SVM Huber Val f1: 0.134 +- 0.268\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in the test data and run against the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test = pull_data(os.path.join(raw_data_path, \"aclImdb/test\"))\n",
    "text_test, y_test = imdb_test.data, imdb_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_df = pd.DataFrame({'text': text_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rows in text_test: 25000, no of rows in y_test: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'No rows in text_test: {len(text_test)}, no of rows in y_test: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in train_id2word\n",
    "with open(id2word_path, 'rb') as f:\n",
    "    train_id2word4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(df):\n",
    "    \"\"\"\n",
    "    For the test data we only need the bigram data built on the TEST reviews,\n",
    "    using the TRAIN id2word mappings. \n",
    "    \"\"\"\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_test = get_bigram(text_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the trained LDA model\n",
    "lda_train4 = gensim.models.ldamulticore.LdaMulticore.load('lda_train4.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the training dict to create a new corpus\n",
    "# Corpus - containing the word id and its frequency in every document\n",
    "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax &amp; Director Lisa Krueger!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                           text\n",
       "0  b\"Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heather_graham'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_id2word[18652]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model from the TRAINING data and then run through unseen test reviews\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(len(text_test)):\n",
    "    top_topics = lda_train4.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "    topic_vec.extend([len(text_test_df.iloc[i].text)]) # length review\n",
    "    test_vecs.append(topic_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791959157626037\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# X now unseen data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "  class_weight= 'balanced',\n",
    "  solver='newton-cg',\n",
    "  fit_intercept=True\n",
    "  ).fit(X, y)\n",
    "\n",
    "y_pred_lr = lr.predict(X)\n",
    "print(f1_score(y, y_pred_lr,average='binary'))\n",
    "\n",
    "sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced',shuffle=True\n",
    "    ).fit(X, y)\n",
    "    \n",
    "y_pred_huber = sgd_huber.predict(X)\n",
    "print(f1_score(y, y_pred_huber, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
